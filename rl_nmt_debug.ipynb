{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import pandas as pd\n",
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    "from tqdm.auto import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# https://docs.wandb.ai/guides/track/environment-variables\n",
    "import os\n",
    "\n",
    "WANDB_PROJECT = 'nmt'\n",
    "WANDB_API_KEY = ''\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'rl_nmt.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n",
    "os.environ['WANDB_PROJECT'] = WANDB_PROJECT\n",
    "\n",
    "os.environ['WANDB_MODE'] = 'offline' #'offline'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide logs from commet\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Suppress info logs\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_text(source, tokenizer):\n",
    "    content = f\"Translate the following English sentence to Ukrainian. Output only the translation without any explanation or extra text.\\n\\nEnglish: {source}\\n\\nUkrainian:\"\n",
    "    message = [\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def process_generated_text(generated_text):\n",
    "    _, _, pred = generated_text.partition(\"Ukrainian:\")\n",
    "    pred = pred.strip()\n",
    "    pred = pred.removeprefix(\"<end_of_turn>\\n<start_of_turn>model\\n\")\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    #quantization_config=quantization_config,\n",
    "    attn_implementation=\"eager\",  # Critical for Gemma3\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dragoman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_prompt_text(source, tokenizer=None):\n",
    "#     prompt = f\"[INST] {source} [/INST]\"\n",
    "#     return prompt\n",
    "\n",
    "# def process_generated_text(generated_text):\n",
    "#     pred = generated_text\n",
    "\n",
    "#     # Remove only the last occurrence of \"</s>\"\n",
    "#     if pred.endswith(\"</s>\"):\n",
    "#         pred = pred[:-len(\"</s>\")]\n",
    "\n",
    "#     pred = pred.split(\"[/INST]\")[-1]\n",
    "\n",
    "#     pred = pred.strip()\n",
    "\n",
    "#     return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install bitsandbytes transformers peft torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from peft import PeftConfig, PeftModel\n",
    "# import torch\n",
    "\n",
    "# config = PeftConfig.from_pretrained(\"lang-uk/dragoman\")\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=False,\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-v0.1\"#, quantization_config=quant_config\n",
    "# )\n",
    "\n",
    "#model = PeftModel.from_pretrained(model, \"lang-uk/dragoman\", is_trainable=True).to(\"cuda\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-v0.1\", use_fast=False, add_bos_token=False\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side='left'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rewards import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "def format_reward_func(completions, sources, references = None, **kwargs):\n",
    "    rewards = []\n",
    " \n",
    "    for completion in completions:\n",
    " \n",
    "      try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion        \n",
    "        # Check if the format is correct\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    " \n",
    "        match = re.search(regex, completion, re.DOTALL) \n",
    "        # if the format is not correct, reward is 0\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(1.0)\n",
    "      except Exception:\n",
    "        rewards.append(0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Load the language ID model\n",
    "model_identify_lang = fasttext.load_model(\"lid.176.bin\")\n",
    "\n",
    "def lang_reward_func(completions, sources=None, references = None, **kwargs):\n",
    "    translations = completions\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for translation in translations:\n",
    "        cleaned = translation.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        # Predict the language of the translation\n",
    "        lang_prediction = model_identify_lang.predict(cleaned)[0][0]\n",
    "        \n",
    "        # Check if the predicted language is Ukrainian\n",
    "        if lang_prediction == \"__label__uk\":\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# texts = [\n",
    "#     \"This is a test sentence.\",\n",
    "#     \"Це тестове речення.\",\n",
    "#     \"Як спрвидливий вчинок? What?\",\n",
    "#     \"Dies ist ein Testsatz.\"\n",
    "# ]  \n",
    "# lang_reward_func(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def translation_reward_func_bleu(completions, sources, references = None, **kwargs):\n",
    "    translations = completions # drop reasning\n",
    "\n",
    "    # Compute BLEU score per sample\n",
    "    bleu_scores = []\n",
    "    for pred, ref in zip(translations, references):\n",
    "        score = bleu.compute(predictions=[pred], references=[ref])['score']\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translation_reward_func_bleu(['Hello how are you doing?'], None, ['Hello how are you?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "comet_model = load_from_checkpoint(model_path)\n",
    "comet_model = comet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_reward_func_reff_free(completions, sources, references = None, **kwargs):\n",
    "    translations = completions # drop reasning\n",
    "\n",
    "    comet_data = [{\"src\": eng, \"mt\": pred} \n",
    "                 for eng, pred in zip(sources, translations)]\n",
    "    \n",
    "    # Compute COMET scores in batch\n",
    "    comet_scores = comet_model.predict(comet_data, batch_size=len(comet_data), progress_bar=False)['scores']\n",
    "\n",
    "    return comet_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translation_reward_func_reff_free(['Як справи'], ['Hello how are you?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name_or_path = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "model_emb_sim = SentenceTransformer(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "# # Paired source and Ukrainian translations\n",
    "# source_texts = [\n",
    "#     \"What is the capital of China?\",\n",
    "#     \"How to implement quick sort in Python?\",\n",
    "# ]\n",
    "\n",
    "# translation_texts = [\n",
    "#     \"Яка столиця Китаю?\",\n",
    "#     \"Як реалізувати швидке сортування в Пайтоні?\"\n",
    "# ]\n",
    "\n",
    "# # Encode texts\n",
    "# source_embeddings = model_emb_sim.encode(source_texts, normalize_embeddings=True)\n",
    "# translation_embeddings = model_emb_sim.encode(translation_texts, normalize_embeddings=True)\n",
    "\n",
    "# # Compute and print pairwise similarity\n",
    "# for i, (src_emb, tgt_emb) in enumerate(zip(source_embeddings, translation_embeddings)):\n",
    "#     score = src_emb @ tgt_emb  # cosine similarity since embeddings are normalized\n",
    "#     print(f\"Pair {i+1}:\")\n",
    "#     print(f\"  Source: {source_texts[i]}\")\n",
    "#     print(f\"  Translation: {translation_texts[i]}\")\n",
    "#     print(f\"  Similarity Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_sim_reward_func_reff_free(completions, sources, references = None, **kwargs):\n",
    "    rewards = []\n",
    "    source_embeddings = model_emb_sim.encode(sources, normalize_embeddings=True)\n",
    "    translation_embeddings = model_emb_sim.encode(completions, normalize_embeddings=True)\n",
    "\n",
    "    for i, (src_emb, tgt_emb) in enumerate(zip(source_embeddings, translation_embeddings)):\n",
    "        score = src_emb @ tgt_emb  # cosine similarity since embeddings are normalized\n",
    "        # print(f\"Pair {i+1}:\")\n",
    "        # print(f\"  Source: {source_texts[i]}\")\n",
    "        # print(f\"  Translation: {translation_texts[i]}\")\n",
    "        # print(f\"  Similarity Score: {score:.4f}\\n\")\n",
    "\n",
    "        rewards.append(score.item())\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(source, reference=None):\n",
    "    datapoint = {\n",
    "        \"sources\": source,\n",
    "        \"references\": reference,\n",
    "        \"prompt\": format_prompt_text(source, tokenizer),\n",
    "    }\n",
    "\n",
    "    return datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def create_dataset_flores(name=\"dev\"): \n",
    "    ds_eng = load_dataset(\"openlanguagedata/flores_plus\", \"eng_Latn\")[name]\n",
    "    ds_ukr = load_dataset(\"openlanguagedata/flores_plus\", \"ukr_Cyrl\")[name]\n",
    "\n",
    "    dataset = []\n",
    "    for eng, ukr in zip(ds_eng, ds_ukr):\n",
    "        datapoint = format_prompt(eng[\"text\"], ukr[\"text\"])\n",
    "        dataset.append(datapoint)\n",
    "\n",
    "    return Dataset.from_list(dataset)\n",
    "\n",
    "train_dataset_flores = create_dataset_flores(name=\"dev\")\n",
    "test_dataset_flores = create_dataset_flores(name=\"devtest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def print_dataset_stats(dataset, name):\n",
    "#     # Extract source and target lengths\n",
    "#     source_lens = [len(item['sources'].split()) for item in dataset]\n",
    "#     target_lens = [len(item['references'].split()) for item in dataset]\n",
    "    \n",
    "#     # Calculate statistics for sources\n",
    "#     source_min = np.min(source_lens)\n",
    "#     source_max = np.max(source_lens)\n",
    "#     source_mean = np.mean(source_lens)\n",
    "#     source_var = np.var(source_lens)\n",
    "    \n",
    "#     # Calculate statistics for targets\n",
    "#     target_min = np.min(target_lens)\n",
    "#     target_max = np.max(target_lens)\n",
    "#     target_mean = np.mean(target_lens)\n",
    "#     target_var = np.var(target_lens)\n",
    "    \n",
    "#     print(f\"\\nStatistics for {name} dataset:\")\n",
    "#     print(\"Sources (English):\")\n",
    "#     print(f\"  Min length: {source_min:.2f} words\")\n",
    "#     print(f\"  Max length: {source_max:.2f} words\")\n",
    "#     print(f\"  Mean length: {source_mean:.2f} words\")\n",
    "#     print(f\"  Variance: {source_var:.2f}\")\n",
    "    \n",
    "#     print(\"\\nTargets (Ukrainian):\")\n",
    "#     print(f\"  Min length: {target_min:.2f} words\")\n",
    "#     print(f\"  Max length: {target_max:.2f} words\")\n",
    "#     print(f\"  Mean length: {target_mean:.2f} words\")\n",
    "#     print(f\"  Variance: {target_var:.2f}\")\n",
    "\n",
    "# # Print statistics for both datasets\n",
    "# print_dataset_stats(train_dataset_flores, \"Train\")\n",
    "# print_dataset_stats(test_dataset_flores, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import statistics\n",
    "\n",
    "# # Load the WikiText-103 dataset\n",
    "# dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")\n",
    "\n",
    "# # Choose the split to analyze: 'train', 'validation', or 'test'\n",
    "# split = 'train'\n",
    "# texts = dataset[split]['text']\n",
    "\n",
    "# # Compute lengths of each text entry\n",
    "# lengths = [len(text.split()) for text in texts if text.strip()]\n",
    "\n",
    "# # Calculate statistics\n",
    "# min_len = min(lengths)\n",
    "# max_len = max(lengths)\n",
    "# mean_len = statistics.mean(lengths)\n",
    "# variance_len = statistics.variance(lengths)\n",
    "\n",
    "# # Display the results\n",
    "# print(f\"Statistics for WikiText-103 ({split} split):\")\n",
    "# print(f\"Minimum length: {min_len} tokens\")\n",
    "# print(f\"Maximum length: {max_len} tokens\")\n",
    "# print(f\"Mean length: {mean_len:.2f} tokens\")\n",
    "# print(f\"Variance: {variance_len:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_wiki(name=\"train\"):\n",
    "    ds_eng = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")[name]\n",
    "\n",
    "    dataset = []\n",
    "    for eng in  tqdm(ds_eng):\n",
    "        datapoint = format_prompt(eng[\"text\"])\n",
    "        dataset.append(datapoint)\n",
    "\n",
    "    return Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = \"train_dataset.pkl\"\n",
    "# test_path = \"test_dataset.pkl\"\n",
    "\n",
    "# if os.path.exists(train_path):\n",
    "#     with open(train_path, \"rb\") as f:\n",
    "#         train_dataset_wiki = pickle.load(f)\n",
    "# else:\n",
    "#     train_dataset_wiki = create_dataset_wiki(name=\"train\")\n",
    "#     with open(\"train_dataset.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(train_dataset_wiki, f)\n",
    "#     print(f\"{train_path} not found.\")\n",
    "\n",
    "# if os.path.exists(test_path):\n",
    "#     with open(test_path, \"rb\") as f:\n",
    "#         test_dataset = pickle.load(f)\n",
    "# else:\n",
    "#     test_dataset_wiki = create_dataset_wiki(name=\"test\")\n",
    "#     with open(\"test_dataset.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(test_dataset_wiki, f)\n",
    "\n",
    "#     print(f\"{test_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_wikipar(name=\"train\"):\n",
    "    streamed_dataset = load_dataset(\"dchaplinsky/wikipar\", split=name, streaming=True)\n",
    "    \n",
    "    n = 10000\n",
    "    ds_eng = [{'text': ex['paragraph_text']} for _, ex in zip(range(n), tqdm(streamed_dataset, total=n))]\n",
    "\n",
    "    dataset = []\n",
    "    for eng in  tqdm(ds_eng):\n",
    "        datapoint = format_prompt(eng[\"text\"])\n",
    "        dataset.append(datapoint)\n",
    "\n",
    "    return Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_wikipar_strat(name=\"train\"):\n",
    "    streamed_dataset = load_dataset(\"dchaplinsky/wikipar-stratified\", split=name)\n",
    "    \n",
    "    dataset = []\n",
    "    for eng in tqdm(streamed_dataset):\n",
    "        datapoint = format_prompt(eng[\"paragraph_text\"])\n",
    "        dataset.append(datapoint)\n",
    "\n",
    "    return Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_wikipar_strat(name=\"train\"):\n",
    "    dataset = load_dataset(\"dchaplinsky/wikipar-stratified\", split=name)\n",
    "\n",
    "    def map_fn(example):\n",
    "        return format_prompt(example[\"paragraph_text\"])\n",
    "\n",
    "    # Apply the map function\n",
    "    mapped_dataset = dataset.map(map_fn)\n",
    "\n",
    "    return mapped_dataset\n",
    "\n",
    "train_dataset_wikipar = create_dataset_wikipar_strat(name=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, TextGenerationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model_comet = load_from_checkpoint(model_path)\n",
    "\n",
    "model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "model_comet_reff_free = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FLORES dataset\n",
    "ds_eng = load_dataset(\"openlanguagedata/flores_plus\", \"eng_Latn\")[\"devtest\"]#.select(range(12))\n",
    "ds_ukr = load_dataset(\"openlanguagedata/flores_plus\", \"ukr_Cyrl\")[\"devtest\"]#.select(range(12))\n",
    "references = [[ex[\"text\"]] for ex in ds_ukr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_flores(model, tokenizer, batch_size_eval, prompts, process_generated_text):\n",
    "    pipe = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), batch_size_eval)):\n",
    "        batch_prompts = prompts[i:i+batch_size_eval]\n",
    "        \n",
    "        # Generate translations for the batch\n",
    "        batch_outputs = pipe(\n",
    "            batch_prompts,\n",
    "            max_new_tokens=256,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            batch_size=batch_size_eval\n",
    "        )\n",
    "        \n",
    "        # Process each output in the batch\n",
    "        for output in batch_outputs:\n",
    "            generated_text = output[0][\"generated_text\"]\n",
    "            pred = process_generated_text(generated_text)\n",
    "            predictions.append(pred)\n",
    "\n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_metrics(predictions, references):\n",
    "    combined_metrics = evaluate.combine([\n",
    "        evaluate.load(\"sacrebleu\"),           # BLEU\n",
    "        evaluate.load(\"bleu\"),                # d-BLEU (classic BLEU)\n",
    "        evaluate.load(\"chrf\"),                # chrF\n",
    "        evaluate.load(\"ter\"),                 # TER\n",
    "    ])\n",
    "    \n",
    "    results = combined_metrics.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    # Log selected metrics only\n",
    "    metrics_to_log = {\n",
    "        \"sacrebleu_score\": results.get(\"sacrebleu_score\"),\n",
    "        \"bleu\": results.get(\"bleu\"),\n",
    "        \"chr_f_score\": results.get(\"chr_f_score\"),\n",
    "        \"ter_score\": results.get(\"ter_score\")\n",
    "    }\n",
    "    \n",
    "    return metrics_to_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comet(predictions):\n",
    "    data = []\n",
    "    \n",
    "    for src, mt, ref in zip(ds_eng, predictions, ds_ukr):\n",
    "        data.append({\n",
    "            \"src\": src[\"text\"],\n",
    "            \"mt\": mt,\n",
    "            \"ref\": ref[\"text\"]\n",
    "        })\n",
    "    \n",
    "    model_output = model_comet.predict(data, batch_size=8, gpus=1)\n",
    "    return {'comet': model_output.system_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cometkiwi(predictions):\n",
    "    data = []\n",
    "    \n",
    "    for src, mt, ref in zip(ds_eng, predictions, ds_ukr):\n",
    "        data.append({\n",
    "            \"src\": src[\"text\"],\n",
    "            \"mt\": mt,\n",
    "            #\"ref\": ref[\"text\"]\n",
    "        })\n",
    "    \n",
    "    model_output = model_comet_reff_free.predict(data, batch_size=8, gpus=1)\n",
    "    return {'cometkiwi': model_output.system_score} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_flores_pipeline(model, tokenizer, batch_size_eval, format_prompt_text, process_generated_text):\n",
    "    # Create input prompts and references\n",
    "    prompts = [format_prompt_text(ex['text'], tokenizer) for ex in ds_eng]\n",
    "    \n",
    "    predictions = run_eval_flores(model, tokenizer, batch_size_eval, prompts, process_generated_text)\n",
    "\n",
    "    m1 = run_basic_metrics(predictions, references)\n",
    "    m2 = run_comet(predictions)\n",
    "    m3 = run_cometkiwi(predictions)\n",
    "    metrics_to_log = m1 | m2 | m3\n",
    "    metrics_to_log = {f\"eval_{k}\": v for k, v in metrics_to_log.items()}\n",
    "\n",
    "    # # Display results\n",
    "    # for metric_name, metric_value in metrics_to_log.items():\n",
    "    #     print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    return metrics_to_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "# https://towardsdatascience.com/customizing-your-fine-tuning-code-using-huggingfaces-transformers-library-65cf2aa806ca/\n",
    "class EvalCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 50 == 0:\n",
    "            print(f\"Step {state.global_step}: running custom logic...\")\n",
    "\n",
    "            model = kwargs[\"model\"]\n",
    "\n",
    "            # Example: Evaluate model on a sample input\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                eval_metrics = run_eval_flores_pipeline(\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    batch_size_eval=8,\n",
    "                    format_prompt_text=format_prompt_text,\n",
    "                    process_generated_text=process_generated_text\n",
    "                )\n",
    "                print('Eval:', eval_metrics)\n",
    "                wandb.log(eval_metrics)\n",
    "\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000000):\n",
    "#     item = train_dataset_wikipar[i]  # Direct access by index\n",
    "#     if len(item['sources']) > 550:\n",
    "#         print(i)\n",
    "#         break\n",
    "\n",
    "# train_dataset_wikipar_single = train_dataset_wikipar.skip(i).take(1)\n",
    "# train_dataset_wikipar_single[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset_wikipar_single = train_dataset_wikipar.skip(35001).take(1)\n",
    "\n",
    "text = \"In recent years, artificial intelligence has rapidly evolved, transforming industries and daily life. From self-driving cars to medical diagnostics, AI systems demonstrate impressive capabilities—but they also raise ethical concerns. Language models, in particular, generate human-like text but may inadvertently reinforce biases present in training data. As researchers strive for transparency and fairness, new evaluation methods aim to capture both accuracy and nuance. Ultimately, responsible AI development depends not only on technical progress, but on thoughtful consideration of its societal impact.\"\n",
    "prompt = format_prompt_text(text, tokenizer)\n",
    "train_dataset_wikipar_single = Dataset.from_list([format_prompt(prompt)])\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Configure training arguments using GRPOConfig\n",
    "training_args = GRPOConfig(\n",
    "    bf16=True,\n",
    "    \n",
    "    num_train_epochs=1000000,\n",
    "    output_dir=\"nmt\",\n",
    "    remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
    "    \n",
    "    learning_rate=1e-6,\n",
    "    num_generations=8,  # 8\n",
    "    per_device_train_batch_size=1, # 16\n",
    "    gradient_accumulation_steps=8, # 16\n",
    "    #per_device_eval_batch_size=4*2,\n",
    "    max_grad_norm=0.4, # 0.5\n",
    "\n",
    "    temperature=0.6,\n",
    "    \n",
    "    # Parameters that control de data preprocessing\n",
    "    max_completion_length=256,\n",
    "    max_prompt_length=256,\n",
    "\n",
    "    # Parameters related to reporting and saving\n",
    "    log_completions=True, \n",
    "    report_to=[\"wandb\"],\n",
    "    logging_steps=1,\n",
    "\n",
    "    #eval_strategy=\"epoch\",\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=2,\n",
    "\n",
    "    #push_to_hub=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    reward_weights=[1.0, 1.0, 1.0, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model, \n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[translation_reward_func_reff_free, emb_sim_reward_func_reff_free, lang_reward_func, reward_lexical, reward_count_uppercase_letters, reward_count_non_letters_no_spaces, reward_count_unpaired_items, reward_count_words, reward_count_unique_words], \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset_wikipar_single,\n",
    "    #callbacks=[EvalCallback()],\n",
    "    #eval_dataset=test_dataset_flores,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"В останні роки штучний інтелект швидко розвивається, перетворюючи галузі та повсякденне життя. Від автономних автомобілів до діагностики в медицині, системи штучного інтелекту демонструють вражаючі можливості, але також викликають етичні занепокоєння. Моделі мови, зокрема, генерують текст, який виглядає людським, але можуть випадково підсилювати упередження, присутні в навчальних даних. З огляду на те, що дослідники прагнуть прозорості та справедливості, нові методи оцінювання намагаються враховувати як точність, так і нюанси. Нарешті, відповідальне розроблення штучного інтелекту залежить не лише від технічного прогресу, а й від обдуманого розгляду його соціального впливу.https://translate.google.com/translate?sl=uk;gl=uk&tl=uk|In recent years, artificial intelligence has rapidly evolved, transforming industries and daily life. From self-driving cars to medical diagnostics, AI systems demonstrate impressive capabilities—but they also raise ethical concerns. Language models, in particular,\"\n",
    "t2 = \"In recent years, artificial intelligence has rapidly evolved, transforming industries and daily life. From self-driving cars to medical diagnostics, AI systems demonstrate impressive capabilities—but they also raise ethical concerns. Language models, in particular, generate human-like text but may inadvertently reinforce biases present in training data. As researchers strive for transparency and fairness, new evaluation methods aim to capture both accuracy and nuance. Ultimately, responsible AI development depends not only on technical progress, but on thoughtful consideration of its societal impact.\"\n",
    "\n",
    "t3 = \"\"\"\n",
    "В останні роки штучний інтелект швидко розвивався, перетворюючи галузі та повсякденне життя. Від автономних автомобілів до діагностики в медицині, системи штучного інтелекту демонструють вражаючі можливості, але також викликають етичні занепокоєння. Моделі мови, зокрема, генерують текст, схожий на людський, але можуть випадково посилювати упередження, присутні в навчальних даних. Як дослідники прагнуть прозорості та справедливості, нові методи оцінки намагаються захопити обидва аспекти – точність та нюанси. Нарешті, відповідальне розроблення штучного інтелекту залежить не лише від технічного прогресу, а від обдуманого розгляду його соціального впливу.अशा प्रकारे.'Interpreted Ukrainian:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "t1=t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = \" In recent years, artificial intelligence has rapidly evolved, transforming industries and daily life. From self-driving cars to medical diagnostics, AI systems demonstrate impressive capabilities—but they also raise ethical concerns. Language models, in particular, generate human-like text but may inadvertently reinforce biases present in training data. As researchers strive for transparency and fairness, new evaluation methods aim to capture both accuracy and nuance. Ultimately, responsible AI development depends not only on technical progress, but on thoughtful consideration of its societal impact.\"\n",
    "t1 = \"У нещодавно минулому штучний інтелект швидко розвивався, перетворюючи галузі та повсякденне життя. Автономні автомобілі, діагностика в медицині, системи штучного інтелекту демонструють вражаючі можливості, але також викликають етичні занепокоєння. Моделі мовлення генерують текст, схожий на людський, але можуть випадково посилювати упередження, які присутні в навчальних даних. Щоб досягти прозорості та справедливості, розробляються нові методи оцінювання, які збирають обидві точність і нюанси. Нарешті, відповідальне розроблення штучного інтелекту залежить не лише від технічного прогресу, але й від обдуманого розгляду його соціального впливу.roneellowஆசை. been\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_non_letters_no_spaces(text):\n",
    "#     preserved = set(\"0123456789()[]{}.:;!?-+=*/%<>$€₴£¥\") # removed ,\n",
    "#     return [char for char in text if char in preserved]\n",
    "\n",
    "# def count_non_letters_no_spaces(text):\n",
    "#     # count = 0\n",
    "#     # for char in text:\n",
    "#     #     if not unicodedata.category(char).startswith('L') and not char.isspace():\n",
    "#     #         count += 1\n",
    "#     # return count\n",
    "\n",
    "#     return len(get_non_letters_no_spaces(text))\n",
    "\n",
    "# def reward_count_non_letters_no_spaces(completions, sources, references = None, **kwargs):\n",
    "#     rewards = []\n",
    "#     for src, comp in zip(sources, completions):\n",
    "#         src_count = count_non_letters_no_spaces(src)\n",
    "#         comp_count = count_non_letters_no_spaces(comp)\n",
    "\n",
    "#         rewards.append(relative_difference(src_count, comp_count))\n",
    "\n",
    "#     return rewards\n",
    "\n",
    "# #reward_count_non_letters_no_spaces([t1], [t2])\n",
    "\n",
    "reward_lexical([t1], [t2]), reward_count_uppercase_letters([t1], [t2]), reward_count_non_letters_no_spaces([t1], [t2]), reward_count_words([t1], [t2]), reward_count_unique_words([t1], [t2]), reward_count_unpaired_items([t1], [t2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.encode(t1)), len(tokenizer.encode(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_non_letters_no_spaces(t1), get_non_letters_no_spaces(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Configure training arguments using GRPOConfig\n",
    "training_args = GRPOConfig(\n",
    "    bf16=True,\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"nmt\",\n",
    "    remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
    "    \n",
    "    learning_rate=1e-6,\n",
    "    num_generations=8,  # 8\n",
    "    per_device_train_batch_size=1, # 16\n",
    "    gradient_accumulation_steps=32, # 16\n",
    "    #per_device_eval_batch_size=4*2,\n",
    "    max_grad_norm=0.4, # 0.5\n",
    "    \n",
    "    # Parameters that control de data preprocessing\n",
    "    max_completion_length=256,\n",
    "    max_prompt_length=256,\n",
    "\n",
    "    # Parameters related to reporting and saving\n",
    "    log_completions=True, \n",
    "    report_to=[\"wandb\"],\n",
    "    logging_steps=10,\n",
    "\n",
    "    #eval_strategy=\"epoch\",\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=2,\n",
    "\n",
    "    #push_to_hub=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    reward_weights=[1.0, 1.0, 1.0, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model, \n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[translation_reward_func_reff_free, emb_sim_reward_func_reff_free, lang_reward_func, reward_lexical, reward_count_uppercase_letters, reward_count_non_letters_no_spaces, reward_count_unpaired_items, reward_count_words, reward_count_unique_words], \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset_wikipar,\n",
    "    callbacks=[EvalCallback()],\n",
    "    #eval_dataset=test_dataset_flores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure training arguments using GRPOConfig\n",
    "# training_args = GRPOConfig(\n",
    "#     output_dir=\"nmt\",\n",
    "#     learning_rate=4e-6,\n",
    "#     remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     gradient_accumulation_steps=16*4,\n",
    "#     num_train_epochs=100,\n",
    "#     bf16=True,\n",
    "#     # Parameters that control de data preprocessing\n",
    "#     max_completion_length=256,  # default: 256\n",
    "#     num_generations=8,  # default: 8\n",
    "#     max_prompt_length=256,  # default: 512\n",
    "#     # Parameters related to reporting and saving\n",
    "#     log_completions=True, \n",
    "#     report_to=[\"wandb\"],\n",
    "#     logging_steps=8,\n",
    "#     eval_strategy=\"epoch\", # eval_steps=4,\n",
    "#     #push_to_hub=True,\n",
    "#     #save_strategy=\"steps\",\n",
    "#     #save_steps=10,\n",
    "# )\n",
    "\n",
    "# trainer = GRPOTrainer(\n",
    "#     model=model, \n",
    "#     reward_funcs=[format_reward_func, translation_reward_func], \n",
    "#     args=training_args, \n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure training arguments using GRPOConfig\n",
    "# training_args = GRPOConfig(\n",
    "#     output_dir=\"nmt\",\n",
    "#     learning_rate=1e-5,\n",
    "#     remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     num_train_epochs=100,\n",
    "#     bf16=True,\n",
    "#     # Parameters that control de data preprocessing\n",
    "#     max_completion_length=256,  # default: 256\n",
    "#     num_generations=8,  # default: 8\n",
    "#     max_prompt_length=256,  # default: 512\n",
    "#     # Parameters related to reporting and saving\n",
    "#     log_completions=True, \n",
    "#     report_to=[\"wandb\"],\n",
    "#     logging_steps=8,\n",
    "#     eval_strategy=\"epoch\", # eval_steps=4,\n",
    "#     #push_to_hub=True,\n",
    "#     #save_strategy=\"steps\",\n",
    "#     #save_steps=10,\n",
    "# )\n",
    "\n",
    "# trainer = GRPOTrainer(\n",
    "#     model=model, \n",
    "#     reward_funcs=[translation_reward_func_reff_free], \n",
    "#     args=training_args, \n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
